{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL: THIS NOTEBOOK TRAINS THE RNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from data_loader import DataLoader\n",
    "from utils.visualize import show_images_in_grid, show_images_as_video, show_reconstructions, compare_images_as_video\n",
    "from cnn import Autoencoder\n",
    "from rnn_initial_state import Seq2Seq\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data_loader = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_data_loader.X_train = np.load(root+\"/../datasets/train/32/X_train_normalized.npy\")\n",
    "rnn_data_loader.X_val = np.load(root+\"/../datasets/val/32/X_val_normalized.npy\")\n",
    "rnn_data_loader.X_test = np.load(root+\"/../datasets/test/32/X_test_normalized.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S COMPUTE THE EMBEDDINGS FROM THE DATA AND TRAINED AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[65536,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dense_3/random_uniform/RandomUniform (defined at /home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4357) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dense_3/random_uniform/RandomUniform', defined at:\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e382e0a780b2>\", line 2, in <module>\n    ae.build_model(input_dim=(32, 32, 3), latent_dim=(64,))\n  File \"/home/abhi/TrackIt/src/cnn.py\", line 77, in build_model\n    d += [Dense(units=32*32*3, activation='sigmoid')(d[-1])]\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 463, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/layers/core.py\", line 895, in build\n    constraint=self.kernel_constraint)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 279, in add_weight\n    weight = K.variable(initializer(shape, dtype=dtype),\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/initializers.py\", line 227, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 4357, in random_uniform\n    shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 4955, in random_uniform\n    shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 247, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 777, in random_uniform\n    name=name)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[65536,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dense_3/random_uniform/RandomUniform (defined at /home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4357) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[65536,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node dense_3/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e382e0a780b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/../models/autoencoder_32.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/TrackIt/src/cnn.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1228\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1230\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1235\u001b[0m                              ' elements.')\n\u001b[1;32m   1236\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1237\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2958\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m     \"\"\"\n\u001b[0;32m-> 2960\u001b[0;31m     \u001b[0mtf_keras_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2878\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2879\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2880\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[65536,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dense_3/random_uniform/RandomUniform (defined at /home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4357) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'dense_3/random_uniform/RandomUniform', defined at:\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e382e0a780b2>\", line 2, in <module>\n    ae.build_model(input_dim=(32, 32, 3), latent_dim=(64,))\n  File \"/home/abhi/TrackIt/src/cnn.py\", line 77, in build_model\n    d += [Dense(units=32*32*3, activation='sigmoid')(d[-1])]\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 463, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/layers/core.py\", line 895, in build\n    constraint=self.kernel_constraint)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/engine/base_layer.py\", line 279, in add_weight\n    weight = K.variable(initializer(shape, dtype=dtype),\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/initializers.py\", line 227, in __call__\n    dtype=dtype, seed=self.seed)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 4357, in random_uniform\n    shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\", line 4955, in random_uniform\n    shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\", line 247, in random_uniform\n    rnd = gen_random_ops.random_uniform(shape, dtype, seed=seed1, seed2=seed2)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/ops/gen_random_ops.py\", line 777, in random_uniform\n    name=name)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[65536,3072] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node dense_3/random_uniform/RandomUniform (defined at /home/abhi/anaconda3/envs/envTF113/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4357) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "ae = Autoencoder()\n",
    "ae.build_model(input_dim=(32, 32, 3), latent_dim=(64,))\n",
    "ae.set_weights(root+\"/../models/autoencoder_32.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = ae.encode_series(rnn_data_loader.X_train)\n",
    "X_val = ae.encode_series(rnn_data_loader.X_val)\n",
    "X_test = ae.encode_series(rnn_data_loader.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = Seq2Seq()\n",
    "seq.build_model(input_dim=64, output_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.set_weights(root+\"/../models/seq2seq_initial_state_v2_32.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 501\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 902us/step - loss: 10.1772 - val_loss: 108.8387\n",
      "EPOCH 502\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 870us/step - loss: 10.1263 - val_loss: 109.0783\n",
      "EPOCH 503\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 869us/step - loss: 10.1560 - val_loss: 109.0719\n",
      "EPOCH 504\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 10.1487 - val_loss: 108.9753\n",
      "EPOCH 505\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 10.0837 - val_loss: 109.0873\n",
      "EPOCH 506\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 10.1290 - val_loss: 109.0227\n",
      "EPOCH 507\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 10.0950 - val_loss: 109.0108\n",
      "EPOCH 508\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 10.0738 - val_loss: 109.0796\n",
      "EPOCH 509\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 10.0812 - val_loss: 108.9531\n",
      "EPOCH 510\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 9.9706 - val_loss: 109.0945\n",
      "EPOCH 511\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 10.0296 - val_loss: 109.1333\n",
      "EPOCH 512\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 10.0311 - val_loss: 109.0208\n",
      "EPOCH 513\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 942us/step - loss: 9.9840 - val_loss: 109.0855\n",
      "EPOCH 514\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 877us/step - loss: 9.8589 - val_loss: 109.0172\n",
      "EPOCH 515\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 9.8772 - val_loss: 109.3557\n",
      "EPOCH 516\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 942us/step - loss: 9.9787 - val_loss: 108.9622\n",
      "EPOCH 517\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 9.9128 - val_loss: 109.0465\n",
      "EPOCH 518\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 9.7231 - val_loss: 108.8847\n",
      "EPOCH 519\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 872us/step - loss: 9.6734 - val_loss: 109.2265\n",
      "EPOCH 520\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 9.6827 - val_loss: 108.7595\n",
      "EPOCH 521\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 9.6092 - val_loss: 109.2164\n",
      "EPOCH 522\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 9.5078 - val_loss: 108.8472\n",
      "EPOCH 523\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 9.5174 - val_loss: 109.2258\n",
      "EPOCH 524\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 9.5593 - val_loss: 108.9725\n",
      "EPOCH 525\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 9.4984 - val_loss: 109.3095\n",
      "EPOCH 526\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 9.5652 - val_loss: 109.2057\n",
      "EPOCH 527\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 9.6282 - val_loss: 109.3692\n",
      "EPOCH 528\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 9.5780 - val_loss: 109.1773\n",
      "EPOCH 529\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 9.5458 - val_loss: 109.4921\n",
      "EPOCH 530\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 9.5135 - val_loss: 109.1260\n",
      "EPOCH 531\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 9.4532 - val_loss: 109.4919\n",
      "EPOCH 532\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 871us/step - loss: 9.4668 - val_loss: 109.1419\n",
      "EPOCH 533\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 9.4331 - val_loss: 109.4417\n",
      "EPOCH 534\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 841us/step - loss: 9.4272 - val_loss: 109.1700\n",
      "EPOCH 535\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 9.4095 - val_loss: 109.5271\n",
      "EPOCH 536\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 880us/step - loss: 9.3370 - val_loss: 109.1374\n",
      "EPOCH 537\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 9.3366 - val_loss: 109.5205\n",
      "EPOCH 538\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 872us/step - loss: 9.3473 - val_loss: 109.1521\n",
      "EPOCH 539\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 9.2571 - val_loss: 109.2406\n",
      "EPOCH 540\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 9.1874 - val_loss: 109.2143\n",
      "EPOCH 541\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 9.1422 - val_loss: 109.3284\n",
      "EPOCH 542\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 9.1558 - val_loss: 109.2564\n",
      "EPOCH 543\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 898us/step - loss: 9.1108 - val_loss: 109.2325\n",
      "EPOCH 544\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 9.0739 - val_loss: 109.4279\n",
      "EPOCH 545\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 998us/step - loss: 9.0249 - val_loss: 109.0118\n",
      "EPOCH 546\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 8.9629 - val_loss: 109.4705\n",
      "EPOCH 547\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 925us/step - loss: 8.9869 - val_loss: 109.4062\n",
      "EPOCH 548\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 9.0218 - val_loss: 109.0213\n",
      "EPOCH 549\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 8.9508 - val_loss: 109.7347\n",
      "EPOCH 550\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 872us/step - loss: 8.9521 - val_loss: 109.0188\n",
      "EPOCH 551\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 8.9166 - val_loss: 109.5062\n",
      "EPOCH 552\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 964us/step - loss: 8.8689 - val_loss: 109.3742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 553\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 8.8858 - val_loss: 109.4162\n",
      "EPOCH 554\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 8.8783 - val_loss: 109.3535\n",
      "EPOCH 555\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 938us/step - loss: 8.9048 - val_loss: 109.4627\n",
      "EPOCH 556\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 8.8200 - val_loss: 109.3793\n",
      "EPOCH 557\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 985us/step - loss: 8.8811 - val_loss: 109.7189\n",
      "EPOCH 558\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 9.0033 - val_loss: 109.4069\n",
      "EPOCH 559\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 8.9754 - val_loss: 109.6834\n",
      "EPOCH 560\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 8.8793 - val_loss: 109.4497\n",
      "EPOCH 561\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 8.8958 - val_loss: 109.7451\n",
      "EPOCH 562\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 8.8893 - val_loss: 109.4740\n",
      "EPOCH 563\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 970us/step - loss: 8.8588 - val_loss: 109.5842\n",
      "EPOCH 564\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 8.8121 - val_loss: 109.5081\n",
      "EPOCH 565\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 873us/step - loss: 8.8435 - val_loss: 109.4342\n",
      "EPOCH 566\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 8.7096 - val_loss: 109.5397\n",
      "EPOCH 567\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 8.6808 - val_loss: 109.3923\n",
      "EPOCH 568\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 869us/step - loss: 8.6832 - val_loss: 109.3376\n",
      "EPOCH 569\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 8.5729 - val_loss: 109.5661\n",
      "EPOCH 570\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 874us/step - loss: 8.5443 - val_loss: 109.3729\n",
      "EPOCH 571\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 869us/step - loss: 8.6206 - val_loss: 109.3293\n",
      "EPOCH 572\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 8.5272 - val_loss: 109.5577\n",
      "EPOCH 573\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 8.5175 - val_loss: 109.1591\n",
      "EPOCH 574\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 888us/step - loss: 8.5998 - val_loss: 109.5530\n",
      "EPOCH 575\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 8.5704 - val_loss: 109.2988\n",
      "EPOCH 576\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 8.5187 - val_loss: 109.5288\n",
      "EPOCH 577\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 987us/step - loss: 8.5639 - val_loss: 109.5573\n",
      "EPOCH 578\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 8.5570 - val_loss: 109.6051\n",
      "EPOCH 579\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 8.5506 - val_loss: 109.3811\n",
      "EPOCH 580\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 8.5080 - val_loss: 109.8316\n",
      "EPOCH 581\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 8.4254 - val_loss: 109.2105\n",
      "EPOCH 582\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 911us/step - loss: 8.4549 - val_loss: 109.8914\n",
      "EPOCH 583\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 8.3887 - val_loss: 109.0509\n",
      "EPOCH 584\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 8.2793 - val_loss: 109.4578\n",
      "EPOCH 585\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 956us/step - loss: 8.1416 - val_loss: 109.2783\n",
      "EPOCH 586\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 8.0828 - val_loss: 109.2464\n",
      "EPOCH 587\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 8.0279 - val_loss: 109.5262\n",
      "EPOCH 588\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 8.0100 - val_loss: 109.2229\n",
      "EPOCH 589\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 871us/step - loss: 7.9782 - val_loss: 109.6251\n",
      "EPOCH 590\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 7.9922 - val_loss: 109.4072\n",
      "EPOCH 591\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 7.9953 - val_loss: 109.5474\n",
      "EPOCH 592\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 950us/step - loss: 7.9499 - val_loss: 109.7141\n",
      "EPOCH 593\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 8.0178 - val_loss: 109.6506\n",
      "EPOCH 594\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 8.0690 - val_loss: 109.7649\n",
      "EPOCH 595\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 8.0312 - val_loss: 109.6716\n",
      "EPOCH 596\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 8.1071 - val_loss: 109.8938\n",
      "EPOCH 597\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 8.0757 - val_loss: 109.4103\n",
      "EPOCH 598\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 7.9433 - val_loss: 109.9523\n",
      "EPOCH 599\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 8.0321 - val_loss: 109.8306\n",
      "EPOCH 600\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 8.0554 - val_loss: 109.8306\n",
      "EPOCH 601\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 7.9645 - val_loss: 109.7924\n",
      "EPOCH 602\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 8.0206 - val_loss: 110.1416\n",
      "EPOCH 603\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 881us/step - loss: 7.9903 - val_loss: 109.5687\n",
      "EPOCH 604\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 963us/step - loss: 7.9365 - val_loss: 110.1092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 605\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 8.0261 - val_loss: 109.7796\n",
      "EPOCH 606\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 7.9473 - val_loss: 109.8074\n",
      "EPOCH 607\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 7.8257 - val_loss: 109.7988\n",
      "EPOCH 608\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 901us/step - loss: 7.8311 - val_loss: 110.0107\n",
      "EPOCH 609\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 7.8316 - val_loss: 109.7306\n",
      "EPOCH 610\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 7.7653 - val_loss: 109.9263\n",
      "EPOCH 611\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 866us/step - loss: 7.7157 - val_loss: 109.8134\n",
      "EPOCH 612\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 7.6914 - val_loss: 110.0248\n",
      "EPOCH 613\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 976us/step - loss: 7.6963 - val_loss: 109.7333\n",
      "EPOCH 614\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 7.6760 - val_loss: 110.2029\n",
      "EPOCH 615\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 7.6189 - val_loss: 109.7668\n",
      "EPOCH 616\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 7.6259 - val_loss: 110.0974\n",
      "EPOCH 617\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 7.5962 - val_loss: 109.9890\n",
      "EPOCH 618\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 7.5787 - val_loss: 109.9998\n",
      "EPOCH 619\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 7.5736 - val_loss: 110.0348\n",
      "EPOCH 620\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 7.5347 - val_loss: 110.0384\n",
      "EPOCH 621\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 7.5276 - val_loss: 110.1713\n",
      "EPOCH 622\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 7.5438 - val_loss: 110.0696\n",
      "EPOCH 623\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 7.5176 - val_loss: 110.2845\n",
      "EPOCH 624\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 7.4891 - val_loss: 110.0770\n",
      "EPOCH 625\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 7.4790 - val_loss: 110.1502\n",
      "EPOCH 626\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 886us/step - loss: 7.3942 - val_loss: 110.1203\n",
      "EPOCH 627\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 7.3923 - val_loss: 110.3399\n",
      "EPOCH 628\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 7.4415 - val_loss: 110.1436\n",
      "EPOCH 629\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 873us/step - loss: 7.4153 - val_loss: 110.4145\n",
      "EPOCH 630\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 7.4289 - val_loss: 110.4371\n",
      "EPOCH 631\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 7.4789 - val_loss: 110.3449\n",
      "EPOCH 632\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 7.5066 - val_loss: 110.5920\n",
      "EPOCH 633\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 988us/step - loss: 7.5761 - val_loss: 110.4525\n",
      "EPOCH 634\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 7.5788 - val_loss: 110.3052\n",
      "EPOCH 635\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 921us/step - loss: 7.5260 - val_loss: 110.6468\n",
      "EPOCH 636\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 7.5869 - val_loss: 110.4547\n",
      "EPOCH 637\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 7.5683 - val_loss: 110.2700\n",
      "EPOCH 638\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 7.5606 - val_loss: 110.7755\n",
      "EPOCH 639\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 7.5895 - val_loss: 110.0331\n",
      "EPOCH 640\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 7.5406 - val_loss: 110.6256\n",
      "EPOCH 641\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 7.4826 - val_loss: 109.9381\n",
      "EPOCH 642\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 982us/step - loss: 7.4605 - val_loss: 110.5074\n",
      "EPOCH 643\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 7.3641 - val_loss: 110.0843\n",
      "EPOCH 644\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 7.3316 - val_loss: 110.4057\n",
      "EPOCH 645\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 7.3005 - val_loss: 110.2774\n",
      "EPOCH 646\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 7.2646 - val_loss: 110.3678\n",
      "EPOCH 647\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 7.2786 - val_loss: 110.2533\n",
      "EPOCH 648\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 7.3165 - val_loss: 110.4423\n",
      "EPOCH 649\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 871us/step - loss: 7.2667 - val_loss: 110.3601\n",
      "EPOCH 650\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 7.3224 - val_loss: 110.4205\n",
      "EPOCH 651\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 901us/step - loss: 7.3383 - val_loss: 110.5209\n",
      "EPOCH 652\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 880us/step - loss: 7.3473 - val_loss: 110.2285\n",
      "EPOCH 653\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 7.2964 - val_loss: 110.5543\n",
      "EPOCH 654\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 7.2557 - val_loss: 110.2626\n",
      "EPOCH 655\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 880us/step - loss: 7.1636 - val_loss: 110.3015\n",
      "EPOCH 656\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 7.0861 - val_loss: 110.5509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 657\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 7.1067 - val_loss: 110.1194\n",
      "EPOCH 658\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 7.0742 - val_loss: 110.4397\n",
      "EPOCH 659\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 876us/step - loss: 7.0029 - val_loss: 110.3468\n",
      "EPOCH 660\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 969us/step - loss: 7.0486 - val_loss: 110.3167\n",
      "EPOCH 661\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 6.9951 - val_loss: 110.2308\n",
      "EPOCH 662\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 6.8464 - val_loss: 110.4186\n",
      "EPOCH 663\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.8610 - val_loss: 110.4135\n",
      "EPOCH 664\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 6.9146 - val_loss: 110.3686\n",
      "EPOCH 665\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 6.8454 - val_loss: 110.5126\n",
      "EPOCH 666\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 6.8269 - val_loss: 110.4996\n",
      "EPOCH 667\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 6.8629 - val_loss: 110.4847\n",
      "EPOCH 668\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 903us/step - loss: 6.8334 - val_loss: 110.4124\n",
      "EPOCH 669\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 6.7748 - val_loss: 110.6470\n",
      "EPOCH 670\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 6.7660 - val_loss: 110.4094\n",
      "EPOCH 671\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.8323 - val_loss: 110.9442\n",
      "EPOCH 672\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 6.7951 - val_loss: 110.3142\n",
      "EPOCH 673\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 6.7358 - val_loss: 111.0446\n",
      "EPOCH 674\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 6.8055 - val_loss: 110.4635\n",
      "EPOCH 675\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 6.8310 - val_loss: 110.8150\n",
      "EPOCH 676\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 894us/step - loss: 6.7030 - val_loss: 110.4906\n",
      "EPOCH 677\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 6.7052 - val_loss: 110.8186\n",
      "EPOCH 678\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 871us/step - loss: 6.7076 - val_loss: 110.6356\n",
      "EPOCH 679\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 6.6922 - val_loss: 110.8058\n",
      "EPOCH 680\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 882us/step - loss: 6.7288 - val_loss: 110.6349\n",
      "EPOCH 681\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 934us/step - loss: 6.6767 - val_loss: 110.9270\n",
      "EPOCH 682\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 6.6599 - val_loss: 110.6623\n",
      "EPOCH 683\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 6.7937 - val_loss: 110.9375\n",
      "EPOCH 684\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 6.7069 - val_loss: 110.7424\n",
      "EPOCH 685\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 6.6691 - val_loss: 110.8396\n",
      "EPOCH 686\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 876us/step - loss: 6.6863 - val_loss: 110.8709\n",
      "EPOCH 687\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 6.6572 - val_loss: 111.1286\n",
      "EPOCH 688\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 947us/step - loss: 6.7673 - val_loss: 110.8803\n",
      "EPOCH 689\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 6.8392 - val_loss: 110.9598\n",
      "EPOCH 690\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 6.6698 - val_loss: 111.0181\n",
      "EPOCH 691\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 945us/step - loss: 6.6296 - val_loss: 110.9564\n",
      "EPOCH 692\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 838us/step - loss: 6.7433 - val_loss: 111.0850\n",
      "EPOCH 693\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 870us/step - loss: 6.7228 - val_loss: 111.0972\n",
      "EPOCH 694\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 6.6291 - val_loss: 110.9023\n",
      "EPOCH 695\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.6229 - val_loss: 111.1039\n",
      "EPOCH 696\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 874us/step - loss: 6.6716 - val_loss: 111.0611\n",
      "EPOCH 697\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 6.5838 - val_loss: 110.8915\n",
      "EPOCH 698\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 6.5307 - val_loss: 111.0746\n",
      "EPOCH 699\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 6.5645 - val_loss: 111.2757\n",
      "EPOCH 700\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 893us/step - loss: 6.5471 - val_loss: 110.8819\n",
      "EPOCH 701\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 6.5043 - val_loss: 111.2072\n",
      "EPOCH 702\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 6.4210 - val_loss: 110.9776\n",
      "EPOCH 703\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 884us/step - loss: 6.3230 - val_loss: 111.2107\n",
      "EPOCH 704\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 6.3314 - val_loss: 111.2234\n",
      "EPOCH 705\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 6.3517 - val_loss: 111.2143\n",
      "EPOCH 706\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 869us/step - loss: 6.3285 - val_loss: 111.2704\n",
      "EPOCH 707\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 888us/step - loss: 6.3048 - val_loss: 111.2283\n",
      "EPOCH 708\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 889us/step - loss: 6.2689 - val_loss: 111.3993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 709\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 898us/step - loss: 6.3135 - val_loss: 111.3354\n",
      "EPOCH 710\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 6.3303 - val_loss: 111.4541\n",
      "EPOCH 711\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 6.2509 - val_loss: 111.4865\n",
      "EPOCH 712\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 890us/step - loss: 6.3431 - val_loss: 111.6641\n",
      "EPOCH 713\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.4757 - val_loss: 111.4517\n",
      "EPOCH 714\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 6.3287 - val_loss: 111.6590\n",
      "EPOCH 715\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 6.3787 - val_loss: 111.6039\n",
      "EPOCH 716\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 6.4953 - val_loss: 111.7227\n",
      "EPOCH 717\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 6.3631 - val_loss: 111.4213\n",
      "EPOCH 718\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.2649 - val_loss: 111.4989\n",
      "EPOCH 719\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 874us/step - loss: 6.2155 - val_loss: 111.6473\n",
      "EPOCH 720\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 870us/step - loss: 6.1938 - val_loss: 111.4198\n",
      "EPOCH 721\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 6.1793 - val_loss: 111.4826\n",
      "EPOCH 722\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 6.1545 - val_loss: 111.7268\n",
      "EPOCH 723\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.1696 - val_loss: 111.5053\n",
      "EPOCH 724\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 6.1747 - val_loss: 111.4915\n",
      "EPOCH 725\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 6.1248 - val_loss: 111.8819\n",
      "EPOCH 726\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 6.2023 - val_loss: 111.5811\n",
      "EPOCH 727\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 872us/step - loss: 6.2171 - val_loss: 111.6185\n",
      "EPOCH 728\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 6.1744 - val_loss: 112.0682\n",
      "EPOCH 729\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 6.2883 - val_loss: 111.5279\n",
      "EPOCH 730\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 6.3398 - val_loss: 111.9946\n",
      "EPOCH 731\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 874us/step - loss: 6.2615 - val_loss: 111.6756\n",
      "EPOCH 732\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 6.2697 - val_loss: 111.7830\n",
      "EPOCH 733\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 6.2224 - val_loss: 111.6587\n",
      "EPOCH 734\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 966us/step - loss: 6.1818 - val_loss: 111.8152\n",
      "EPOCH 735\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 6.1444 - val_loss: 111.7114\n",
      "EPOCH 736\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 6.1937 - val_loss: 111.8396\n",
      "EPOCH 737\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 6.2815 - val_loss: 111.9147\n",
      "EPOCH 738\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 6.1819 - val_loss: 111.5157\n",
      "EPOCH 739\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 6.0894 - val_loss: 111.9892\n",
      "EPOCH 740\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 6.0428 - val_loss: 111.5476\n",
      "EPOCH 741\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 905us/step - loss: 6.0052 - val_loss: 111.8171\n",
      "EPOCH 742\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 865us/step - loss: 5.9668 - val_loss: 111.5364\n",
      "EPOCH 743\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 839us/step - loss: 5.9225 - val_loss: 111.8727\n",
      "EPOCH 744\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 5.8738 - val_loss: 111.6142\n",
      "EPOCH 745\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 949us/step - loss: 5.8631 - val_loss: 111.8489\n",
      "EPOCH 746\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.8517 - val_loss: 111.8759\n",
      "EPOCH 747\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 948us/step - loss: 5.8378 - val_loss: 111.7916\n",
      "EPOCH 748\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 5.8811 - val_loss: 111.9363\n",
      "EPOCH 749\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 5.8625 - val_loss: 111.8217\n",
      "EPOCH 750\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 930us/step - loss: 5.7876 - val_loss: 111.8788\n",
      "EPOCH 751\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 5.7922 - val_loss: 111.9664\n",
      "EPOCH 752\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.8182 - val_loss: 112.0200\n",
      "EPOCH 753\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 5.7408 - val_loss: 111.7376\n",
      "EPOCH 754\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 879us/step - loss: 5.7441 - val_loss: 112.4380\n",
      "EPOCH 755\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 5.8057 - val_loss: 111.5845\n",
      "EPOCH 756\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 5.8070 - val_loss: 112.4491\n",
      "EPOCH 757\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 900us/step - loss: 5.7476 - val_loss: 111.7381\n",
      "EPOCH 758\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 5.7518 - val_loss: 112.3084\n",
      "EPOCH 759\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 5.7442 - val_loss: 111.9707\n",
      "EPOCH 760\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 5.7324 - val_loss: 112.1815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 761\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.6883 - val_loss: 111.9070\n",
      "EPOCH 762\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 5.6596 - val_loss: 112.2677\n",
      "EPOCH 763\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 922us/step - loss: 5.6445 - val_loss: 111.9162\n",
      "EPOCH 764\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 5.6804 - val_loss: 112.3736\n",
      "EPOCH 765\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 5.6779 - val_loss: 111.9941\n",
      "EPOCH 766\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 872us/step - loss: 5.6655 - val_loss: 112.3359\n",
      "EPOCH 767\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 5.6754 - val_loss: 112.2553\n",
      "EPOCH 768\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 5.7229 - val_loss: 112.2885\n",
      "EPOCH 769\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 895us/step - loss: 5.7423 - val_loss: 112.3787\n",
      "EPOCH 770\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 5.7329 - val_loss: 112.2423\n",
      "EPOCH 771\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 872us/step - loss: 5.7185 - val_loss: 112.2964\n",
      "EPOCH 772\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 887us/step - loss: 5.7223 - val_loss: 112.3805\n",
      "EPOCH 773\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 892us/step - loss: 5.6749 - val_loss: 112.1293\n",
      "EPOCH 774\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 841us/step - loss: 5.6203 - val_loss: 112.4370\n",
      "EPOCH 775\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 932us/step - loss: 5.6256 - val_loss: 112.3564\n",
      "EPOCH 776\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.6715 - val_loss: 112.1279\n",
      "EPOCH 777\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 5.5906 - val_loss: 112.4444\n",
      "EPOCH 778\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 892us/step - loss: 5.5022 - val_loss: 112.1751\n",
      "EPOCH 779\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 5.5579 - val_loss: 112.5422\n",
      "EPOCH 780\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 975us/step - loss: 5.5866 - val_loss: 112.1809\n",
      "EPOCH 781\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 5.5178 - val_loss: 112.5378\n",
      "EPOCH 782\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 5.4978 - val_loss: 112.2943\n",
      "EPOCH 783\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 944us/step - loss: 5.5107 - val_loss: 112.5640\n",
      "EPOCH 784\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 917us/step - loss: 5.5288 - val_loss: 112.4377\n",
      "EPOCH 785\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 5.4890 - val_loss: 112.5305\n",
      "EPOCH 786\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 5.4514 - val_loss: 112.5515\n",
      "EPOCH 787\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.4956 - val_loss: 112.8196\n",
      "EPOCH 788\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 868us/step - loss: 5.6075 - val_loss: 112.5877\n",
      "EPOCH 789\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 5.6109 - val_loss: 112.7173\n",
      "EPOCH 790\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 965us/step - loss: 5.5555 - val_loss: 112.6619\n",
      "EPOCH 791\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 5.5781 - val_loss: 112.8322\n",
      "EPOCH 792\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 891us/step - loss: 5.7232 - val_loss: 112.6957\n",
      "EPOCH 793\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 950us/step - loss: 5.6151 - val_loss: 112.4533\n",
      "EPOCH 794\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 5.4786 - val_loss: 112.8743\n",
      "EPOCH 795\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 5.5096 - val_loss: 112.3781\n",
      "EPOCH 796\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 5.5299 - val_loss: 112.7903\n",
      "EPOCH 797\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 5.4535 - val_loss: 112.2723\n",
      "EPOCH 798\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.4326 - val_loss: 112.9816\n",
      "EPOCH 799\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 5.4532 - val_loss: 112.2198\n",
      "EPOCH 800\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 866us/step - loss: 5.4478 - val_loss: 113.0982\n",
      "EPOCH 801\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 871us/step - loss: 5.4864 - val_loss: 112.5534\n",
      "EPOCH 802\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 5.5269 - val_loss: 112.7195\n",
      "EPOCH 803\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 5.4825 - val_loss: 112.7146\n",
      "EPOCH 804\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 874us/step - loss: 5.4969 - val_loss: 112.9183\n",
      "EPOCH 805\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 5.4731 - val_loss: 112.3220\n",
      "EPOCH 806\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 890us/step - loss: 5.4062 - val_loss: 113.2138\n",
      "EPOCH 807\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 5.4721 - val_loss: 112.4485\n",
      "EPOCH 808\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 5.5188 - val_loss: 113.0188\n",
      "EPOCH 809\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 965us/step - loss: 5.3791 - val_loss: 112.4724\n",
      "EPOCH 810\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 5.3613 - val_loss: 113.1430\n",
      "EPOCH 811\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 5.3662 - val_loss: 112.5121\n",
      "EPOCH 812\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 5.3368 - val_loss: 112.9899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 813\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 5.3099 - val_loss: 112.7242\n",
      "EPOCH 814\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 889us/step - loss: 5.3258 - val_loss: 112.9056\n",
      "EPOCH 815\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 5.3172 - val_loss: 112.7138\n",
      "EPOCH 816\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 964us/step - loss: 5.3595 - val_loss: 113.1160\n",
      "EPOCH 817\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 5.3469 - val_loss: 112.5862\n",
      "EPOCH 818\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 965us/step - loss: 5.4020 - val_loss: 113.1263\n",
      "EPOCH 819\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 5.4702 - val_loss: 112.7078\n",
      "EPOCH 820\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 5.3930 - val_loss: 112.7525\n",
      "EPOCH 821\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 5.3162 - val_loss: 112.9053\n",
      "EPOCH 822\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 839us/step - loss: 5.4479 - val_loss: 112.7668\n",
      "EPOCH 823\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 5.5065 - val_loss: 112.6228\n",
      "EPOCH 824\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 5.3245 - val_loss: 112.7726\n",
      "EPOCH 825\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 952us/step - loss: 5.2691 - val_loss: 112.7345\n",
      "EPOCH 826\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 957us/step - loss: 5.3724 - val_loss: 112.8853\n",
      "EPOCH 827\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 5.3105 - val_loss: 112.5814\n",
      "EPOCH 828\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 5.1914 - val_loss: 112.9033\n",
      "EPOCH 829\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 918us/step - loss: 5.1885 - val_loss: 112.6915\n",
      "EPOCH 830\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 5.2224 - val_loss: 113.0315\n",
      "EPOCH 831\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 994us/step - loss: 5.2351 - val_loss: 112.5118\n",
      "EPOCH 832\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 5.1681 - val_loss: 113.1913\n",
      "EPOCH 833\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 846us/step - loss: 5.1857 - val_loss: 112.7274\n",
      "EPOCH 834\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 5.2421 - val_loss: 113.0217\n",
      "EPOCH 835\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 876us/step - loss: 5.1710 - val_loss: 112.7778\n",
      "EPOCH 836\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 5.1036 - val_loss: 113.1243\n",
      "EPOCH 837\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 5.0825 - val_loss: 112.6877\n",
      "EPOCH 838\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 5.1063 - val_loss: 113.3553\n",
      "EPOCH 839\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 889us/step - loss: 5.1160 - val_loss: 112.6862\n",
      "EPOCH 840\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 993us/step - loss: 5.0959 - val_loss: 113.2826\n",
      "EPOCH 841\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 838us/step - loss: 5.0821 - val_loss: 113.0122\n",
      "EPOCH 842\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.2044 - val_loss: 113.3872\n",
      "EPOCH 843\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 5.2111 - val_loss: 112.8129\n",
      "EPOCH 844\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 944us/step - loss: 5.0858 - val_loss: 113.4105\n",
      "EPOCH 845\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 5.0542 - val_loss: 112.9190\n",
      "EPOCH 846\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 5.0954 - val_loss: 113.3138\n",
      "EPOCH 847\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 903us/step - loss: 5.0780 - val_loss: 113.1063\n",
      "EPOCH 848\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 838us/step - loss: 5.0546 - val_loss: 113.1919\n",
      "EPOCH 849\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 5.0543 - val_loss: 113.3715\n",
      "EPOCH 850\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 5.0245 - val_loss: 113.0960\n",
      "EPOCH 851\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 5.0259 - val_loss: 113.3641\n",
      "EPOCH 852\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 4.9868 - val_loss: 113.1561\n",
      "EPOCH 853\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 918us/step - loss: 4.9482 - val_loss: 113.2951\n",
      "EPOCH 854\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 940us/step - loss: 4.9426 - val_loss: 113.2650\n",
      "EPOCH 855\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 4.9428 - val_loss: 113.2379\n",
      "EPOCH 856\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 884us/step - loss: 4.9074 - val_loss: 113.3601\n",
      "EPOCH 857\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 4.9191 - val_loss: 113.2693\n",
      "EPOCH 858\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 974us/step - loss: 4.9173 - val_loss: 113.4818\n",
      "EPOCH 859\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 4.8503 - val_loss: 113.1507\n",
      "EPOCH 860\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 914us/step - loss: 4.7942 - val_loss: 113.7093\n",
      "EPOCH 861\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 4.8472 - val_loss: 113.1596\n",
      "EPOCH 862\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 926us/step - loss: 4.8680 - val_loss: 113.7628\n",
      "EPOCH 863\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 4.8352 - val_loss: 113.2872\n",
      "EPOCH 864\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 4.9079 - val_loss: 114.0553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 865\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 4.9939 - val_loss: 113.2234\n",
      "EPOCH 866\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 4.9368 - val_loss: 114.1399\n",
      "EPOCH 867\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 916us/step - loss: 4.9499 - val_loss: 113.4379\n",
      "EPOCH 868\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 4.9862 - val_loss: 113.8252\n",
      "EPOCH 869\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 4.9368 - val_loss: 113.6742\n",
      "EPOCH 870\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 894us/step - loss: 4.8768 - val_loss: 113.5280\n",
      "EPOCH 871\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 4.8306 - val_loss: 113.6088\n",
      "EPOCH 872\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 4.7775 - val_loss: 113.4757\n",
      "EPOCH 873\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 4.7475 - val_loss: 113.6218\n",
      "EPOCH 874\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.7332 - val_loss: 113.4385\n",
      "EPOCH 875\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 908us/step - loss: 4.7213 - val_loss: 113.7484\n",
      "EPOCH 876\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 933us/step - loss: 4.7356 - val_loss: 113.4863\n",
      "EPOCH 877\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 843us/step - loss: 4.7206 - val_loss: 113.6643\n",
      "EPOCH 878\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 836us/step - loss: 4.7034 - val_loss: 113.7703\n",
      "EPOCH 879\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 904us/step - loss: 4.6766 - val_loss: 113.4967\n",
      "EPOCH 880\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 838us/step - loss: 4.6883 - val_loss: 114.0157\n",
      "EPOCH 881\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.6859 - val_loss: 113.4579\n",
      "EPOCH 882\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 874us/step - loss: 4.6790 - val_loss: 114.1840\n",
      "EPOCH 883\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.6741 - val_loss: 113.5454\n",
      "EPOCH 884\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.7391 - val_loss: 114.2636\n",
      "EPOCH 885\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 4.6992 - val_loss: 113.5397\n",
      "EPOCH 886\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 954us/step - loss: 4.6441 - val_loss: 114.1988\n",
      "EPOCH 887\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 992us/step - loss: 4.6525 - val_loss: 113.7603\n",
      "EPOCH 888\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.6499 - val_loss: 114.2241\n",
      "EPOCH 889\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 837us/step - loss: 4.6733 - val_loss: 113.7858\n",
      "EPOCH 890\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 916us/step - loss: 4.7731 - val_loss: 114.4842\n",
      "EPOCH 891\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 4.7500 - val_loss: 113.5615\n",
      "EPOCH 892\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 971us/step - loss: 4.7362 - val_loss: 114.5247\n",
      "EPOCH 893\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 4.7289 - val_loss: 113.8394\n",
      "EPOCH 894\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 865us/step - loss: 4.7443 - val_loss: 114.2922\n",
      "EPOCH 895\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 4.7518 - val_loss: 114.0360\n",
      "EPOCH 896\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 4.7066 - val_loss: 114.1227\n",
      "EPOCH 897\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 4.6563 - val_loss: 114.1270\n",
      "EPOCH 898\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 4.6513 - val_loss: 114.0836\n",
      "EPOCH 899\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 931us/step - loss: 4.5875 - val_loss: 114.0711\n",
      "EPOCH 900\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 993us/step - loss: 4.5458 - val_loss: 114.1306\n",
      "EPOCH 901\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 918us/step - loss: 4.5351 - val_loss: 114.0624\n",
      "EPOCH 902\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 891us/step - loss: 4.5355 - val_loss: 114.2671\n",
      "EPOCH 903\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 4.5545 - val_loss: 114.0913\n",
      "EPOCH 904\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.5540 - val_loss: 114.2507\n",
      "EPOCH 905\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 4.5451 - val_loss: 114.2586\n",
      "EPOCH 906\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 877us/step - loss: 4.5828 - val_loss: 114.2989\n",
      "EPOCH 907\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 868us/step - loss: 4.6029 - val_loss: 114.2538\n",
      "EPOCH 908\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 4.5453 - val_loss: 114.2358\n",
      "EPOCH 909\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 997us/step - loss: 4.5196 - val_loss: 114.2801\n",
      "EPOCH 910\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 4.4948 - val_loss: 114.2808\n",
      "EPOCH 911\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 945us/step - loss: 4.4615 - val_loss: 114.1628\n",
      "EPOCH 912\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.4644 - val_loss: 114.4772\n",
      "EPOCH 913\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.4505 - val_loss: 114.0669\n",
      "EPOCH 914\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 838us/step - loss: 4.4176 - val_loss: 114.4528\n",
      "EPOCH 915\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 978us/step - loss: 4.4270 - val_loss: 114.1908\n",
      "EPOCH 916\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 4.4300 - val_loss: 114.5336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 917\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 4.4331 - val_loss: 114.1213\n",
      "EPOCH 918\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 910us/step - loss: 4.4314 - val_loss: 114.7026\n",
      "EPOCH 919\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 865us/step - loss: 4.4368 - val_loss: 114.2358\n",
      "EPOCH 920\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 963us/step - loss: 4.4677 - val_loss: 114.5693\n",
      "EPOCH 921\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 861us/step - loss: 4.4914 - val_loss: 114.3592\n",
      "EPOCH 922\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.4596 - val_loss: 114.5764\n",
      "EPOCH 923\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.4875 - val_loss: 114.3096\n",
      "EPOCH 924\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 4.5344 - val_loss: 114.6888\n",
      "EPOCH 925\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 929us/step - loss: 4.4742 - val_loss: 114.1962\n",
      "EPOCH 926\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.4539 - val_loss: 114.7136\n",
      "EPOCH 927\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 4.4518 - val_loss: 114.0840\n",
      "EPOCH 928\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 4.4487 - val_loss: 114.8011\n",
      "EPOCH 929\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 4.3728 - val_loss: 113.8327\n",
      "EPOCH 930\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 859us/step - loss: 4.3495 - val_loss: 115.1033\n",
      "EPOCH 931\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 855us/step - loss: 4.4140 - val_loss: 113.7945\n",
      "EPOCH 932\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 4.4300 - val_loss: 115.1420\n",
      "EPOCH 933\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 873us/step - loss: 4.4049 - val_loss: 113.8399\n",
      "EPOCH 934\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.4795 - val_loss: 115.2973\n",
      "EPOCH 935\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 4.5136 - val_loss: 113.8336\n",
      "EPOCH 936\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 4.4853 - val_loss: 115.1395\n",
      "EPOCH 937\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 4.5071 - val_loss: 114.2164\n",
      "EPOCH 938\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 4.4964 - val_loss: 114.8557\n",
      "EPOCH 939\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.4921 - val_loss: 114.4058\n",
      "EPOCH 940\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 4.4695 - val_loss: 114.8099\n",
      "EPOCH 941\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 847us/step - loss: 4.4175 - val_loss: 114.1684\n",
      "EPOCH 942\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 925us/step - loss: 4.3833 - val_loss: 114.8093\n",
      "EPOCH 943\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 4.3276 - val_loss: 114.1338\n",
      "EPOCH 944\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 4.2839 - val_loss: 114.7715\n",
      "EPOCH 945\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 876us/step - loss: 4.2735 - val_loss: 114.2831\n",
      "EPOCH 946\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 4.2782 - val_loss: 114.7026\n",
      "EPOCH 947\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 4.2797 - val_loss: 114.5200\n",
      "EPOCH 948\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 941us/step - loss: 4.3331 - val_loss: 114.6655\n",
      "EPOCH 949\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.2999 - val_loss: 114.5353\n",
      "EPOCH 950\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 4.2753 - val_loss: 114.8389\n",
      "EPOCH 951\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 856us/step - loss: 4.3367 - val_loss: 114.4709\n",
      "EPOCH 952\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 849us/step - loss: 4.3431 - val_loss: 114.9131\n",
      "EPOCH 953\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 914us/step - loss: 4.2535 - val_loss: 114.4741\n",
      "EPOCH 954\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.2187 - val_loss: 114.8395\n",
      "EPOCH 955\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 4.2230 - val_loss: 114.6233\n",
      "EPOCH 956\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 934us/step - loss: 4.1900 - val_loss: 114.7956\n",
      "EPOCH 957\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 906us/step - loss: 4.2000 - val_loss: 114.9005\n",
      "EPOCH 958\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 869us/step - loss: 4.2084 - val_loss: 114.5720\n",
      "EPOCH 959\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.1668 - val_loss: 115.0354\n",
      "EPOCH 960\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 863us/step - loss: 4.1758 - val_loss: 114.7782\n",
      "EPOCH 961\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 865us/step - loss: 4.2248 - val_loss: 114.8158\n",
      "EPOCH 962\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 922us/step - loss: 4.2153 - val_loss: 115.0220\n",
      "EPOCH 963\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 914us/step - loss: 4.1717 - val_loss: 114.7143\n",
      "EPOCH 964\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 839us/step - loss: 4.1827 - val_loss: 115.0549\n",
      "EPOCH 965\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 870us/step - loss: 4.1568 - val_loss: 114.6583\n",
      "EPOCH 966\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.1535 - val_loss: 115.4190\n",
      "EPOCH 967\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 864us/step - loss: 4.1819 - val_loss: 114.4071\n",
      "EPOCH 968\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 886us/step - loss: 4.2144 - val_loss: 115.6843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 969\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 4.2114 - val_loss: 114.5451\n",
      "EPOCH 970\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 4.1908 - val_loss: 115.4748\n",
      "EPOCH 971\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 968us/step - loss: 4.2160 - val_loss: 115.0281\n",
      "EPOCH 972\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.2501 - val_loss: 115.1962\n",
      "EPOCH 973\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 852us/step - loss: 4.2174 - val_loss: 115.0521\n",
      "EPOCH 974\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 860us/step - loss: 4.2053 - val_loss: 115.2801\n",
      "EPOCH 975\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 842us/step - loss: 4.2086 - val_loss: 115.0181\n",
      "EPOCH 976\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 4.1696 - val_loss: 115.4383\n",
      "EPOCH 977\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 848us/step - loss: 4.1583 - val_loss: 115.0896\n",
      "EPOCH 978\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 840us/step - loss: 4.2031 - val_loss: 115.4553\n",
      "EPOCH 979\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 4.1305 - val_loss: 115.0681\n",
      "EPOCH 980\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 4.0869 - val_loss: 115.3470\n",
      "EPOCH 981\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 844us/step - loss: 4.1099 - val_loss: 115.3169\n",
      "EPOCH 982\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 845us/step - loss: 4.1152 - val_loss: 115.1124\n",
      "EPOCH 983\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 854us/step - loss: 4.0514 - val_loss: 115.2668\n",
      "EPOCH 984\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 851us/step - loss: 4.0507 - val_loss: 115.3847\n",
      "EPOCH 985\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 858us/step - loss: 4.0551 - val_loss: 115.1185\n",
      "EPOCH 986\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 879us/step - loss: 4.0856 - val_loss: 115.5519\n",
      "EPOCH 987\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 871us/step - loss: 4.0696 - val_loss: 115.3038\n",
      "EPOCH 988\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 862us/step - loss: 4.0722 - val_loss: 115.3531\n",
      "EPOCH 989\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 985us/step - loss: 4.0721 - val_loss: 115.4782\n",
      "EPOCH 990\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 866us/step - loss: 4.0877 - val_loss: 115.3248\n",
      "EPOCH 991\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.0747 - val_loss: 115.3304\n",
      "EPOCH 992\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 890us/step - loss: 4.0724 - val_loss: 115.4188\n",
      "EPOCH 993\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 960us/step - loss: 4.0849 - val_loss: 115.4693\n",
      "EPOCH 994\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 889us/step - loss: 4.1520 - val_loss: 115.3605\n",
      "EPOCH 995\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 857us/step - loss: 4.1057 - val_loss: 115.5217\n",
      "EPOCH 996\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 867us/step - loss: 4.0747 - val_loss: 115.3668\n",
      "EPOCH 997\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 850us/step - loss: 4.0947 - val_loss: 115.6274\n",
      "EPOCH 998\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 896us/step - loss: 4.1039 - val_loss: 115.2417\n",
      "EPOCH 999\n",
      "Train on 160 samples, validate on 20 samples\n",
      "Epoch 1/1\n",
      "160/160 [==============================] - 0s 853us/step - loss: 4.0633 - val_loss: 115.6697\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN LOOP\n",
    "for epoch in range(1000):\n",
    "    print('EPOCH %d' % (epoch))\n",
    "    seq.train(X_train=X_train[:, :49, :], \n",
    "             Y_train=X_train[:, 1:50, :], \n",
    "             X_val=X_val[:, :49, :], \n",
    "             Y_val=X_val[:, 1:50, :], \n",
    "             epochs=1, \n",
    "             batch_size=32\n",
    "             )\n",
    "    if epoch % 500 == 0:\n",
    "        i = random.randint(0, X_train.shape[0]-1)\n",
    "        X, Y = [], []\n",
    "        x = X_train[i, :49, :]\n",
    "        X.append(x)\n",
    "        X = np.array(X)\n",
    "        y = X_train[i, 1:50, :]\n",
    "        Y.append(y)\n",
    "        Y = np.array(Y)\n",
    "        Y_hat = seq.model.predict(X)\n",
    "        Y_decoded = ae.decode_series(Y)\n",
    "        Y_hat_decoded = ae.decode_series(Y_hat)\n",
    "        compare_images_as_video(Y_decoded[0], Y_hat_decoded[0])\n",
    "    seq.save_weights(root+\"/../models/seq2seq_initial_state_v2_32.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_videos_new(train=True, val=False, test=False, i=None):\n",
    "    if i == None:\n",
    "        if train:\n",
    "            i = random.randint(0, X_train.shape[0]-1)\n",
    "        elif val:\n",
    "            i = random.randint(0, X_val.shape[0]-1)\n",
    "        elif test:\n",
    "            i = random.randint(0, X_test.shape[0]-1)\n",
    "    X, Y = [], []\n",
    "    x, y = None, None\n",
    "    h, c = np.array([np.zeros(1024)]), np.array([np.zeros(1024)])\n",
    "    if train:\n",
    "        x = X_train[i, 0, :]\n",
    "        y = X_train[i, 1:51, :]\n",
    "    elif val:\n",
    "        x = X_val[i, 0, :]\n",
    "        y = X_val[i, 1:51, :]\n",
    "    elif test:\n",
    "        x = X_test[i, 0, :]\n",
    "        y = X_test[i, 1:51, :]\n",
    "    \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = np.expand_dims(x, axis=1)\n",
    "    for _ in range(100):\n",
    "        x_new, h_new, c_new = seq.forward.predict([x, h, c])\n",
    "        # print(x_new.shape, h_new.shape, c_new.shape)\n",
    "        # print(x_new.shape)\n",
    "        X.append(x_new)\n",
    "        x, h, c = x_new, h_new, c_new\n",
    "        \n",
    "    X = np.array(X)\n",
    "    Y.append(y)\n",
    "    Y = np.array(Y)\n",
    "    X = np.reshape(X, (1, 100, 64))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    Y_hat_decoded = ae.decode_series(X)\n",
    "    Y_decoded = ae.decode_series(Y)\n",
    "    print(Y_decoded.shape)\n",
    "    print(Y_hat_decoded.shape)\n",
    "#     result = np.concatenate((X, Y), axis=1)\n",
    "#     Y_decoded = ae.decode_series(result)\n",
    "    return Y_decoded[0], Y_hat_decoded[0], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_videos_new_two(train=True, val=False, test=False, i=None):\n",
    "    if i == None:\n",
    "        if train:\n",
    "            i = random.randint(0, X_train.shape[0]-1)\n",
    "        elif val:\n",
    "            i = random.randint(0, X_val.shape[0]-1)\n",
    "        elif test:\n",
    "            i = random.randint(0, X_test.shape[0]-1)\n",
    "    X, Y = [], []\n",
    "    x, y = None, None\n",
    "    h, c = np.array([np.zeros(1024)]), np.array([np.zeros(1024)])\n",
    "    if train:\n",
    "        x = X_train[i, 0, :]\n",
    "        y = X_train[i, 1:51, :]\n",
    "    elif val:\n",
    "        x = X_val[i, 0, :]\n",
    "        y = X_val[i, 1:51, :]\n",
    "    elif test:\n",
    "        x = X_test[i, 0, :]\n",
    "        y = X_test[i, 1:51, :]\n",
    "    \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = np.expand_dims(x, axis=1)\n",
    "    for _ in range(100):\n",
    "        x_new = seq.model.predict(x)\n",
    "        # print(x_new.shape, h_new.shape, c_new.shape)\n",
    "        # print(x_new.shape)\n",
    "        X.append(x_new)\n",
    "        x = x_new\n",
    "        \n",
    "    X = np.array(X)\n",
    "    Y.append(y)\n",
    "    Y = np.array(Y)\n",
    "    X = np.reshape(X, (1, 100, 64))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    Y_hat_decoded = ae.decode_series(X)\n",
    "    Y_decoded = ae.decode_series(Y)\n",
    "    print(Y_decoded.shape)\n",
    "    print(Y_hat_decoded.shape)\n",
    "#     result = np.concatenate((X, Y), axis=1)\n",
    "#     Y_decoded = ae.decode_series(result)\n",
    "    return Y_decoded[0], Y_hat_decoded[0], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_videos_new_three(train=True, val=False, test=False, i=None):\n",
    "    if i == None:\n",
    "        if train:\n",
    "            i = random.randint(0, X_train.shape[0]-1)\n",
    "        elif val:\n",
    "            i = random.randint(0, X_val.shape[0]-1)\n",
    "        elif test:\n",
    "            i = random.randint(0, X_test.shape[0]-1)\n",
    "    X, Y = [], []\n",
    "    x, y = None, None\n",
    "    h, c = np.array([np.zeros(1024)]), np.array([np.zeros(1024)])\n",
    "    if train:\n",
    "        x = X_train[i, 0, :]\n",
    "        y = X_train[i, 1:51, :]\n",
    "    elif val:\n",
    "        x = X_val[i, 0, :]\n",
    "        y = X_val[i, 1:51, :]\n",
    "    elif test:\n",
    "        x = X_test[i, 0, :]\n",
    "        y = X_test[i, 1:51, :]\n",
    "    \n",
    "\n",
    "    for j in range(50):\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = np.expand_dims(x, axis=1)\n",
    "        x_new, h_new, c_new = seq.forward.predict([x, h, c])\n",
    "        # print(x_new.shape, h_new.shape, c_new.shape)\n",
    "        # print(x_new.shape)\n",
    "        X.append(x_new)\n",
    "        x, h, c = X_train[i, j+1, :], h_new, c_new\n",
    "        \n",
    "    X = np.array(X)\n",
    "    Y.append(y)\n",
    "    Y = np.array(Y)\n",
    "    X = np.reshape(X, (1, 50, 64))\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    Y_hat_decoded = ae.decode_series(X)\n",
    "    Y_decoded = ae.decode_series(Y)\n",
    "    print(Y_decoded.shape)\n",
    "    print(Y_hat_decoded.shape)\n",
    "#     result = np.concatenate((X, Y), axis=1)\n",
    "#     Y_decoded = ae.decode_series(result)\n",
    "    return Y_decoded[0], Y_hat_decoded[0], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_videos(train=True, val=False, test=False, i=None):\n",
    "    if i == None:\n",
    "        if train:\n",
    "            i = random.randint(0, X_train.shape[0]-1)\n",
    "        elif val:\n",
    "            i = random.randint(0, X_val.shape[0]-1)\n",
    "        elif test:\n",
    "            i = random.randint(0, X_test.shape[0]-1)\n",
    "    X, Y = [], []\n",
    "    x, y = None, None\n",
    "    if train:\n",
    "        x = X_train[i, :49, :]\n",
    "        y = X_train[i, 1:50, :]\n",
    "    elif val:\n",
    "        x = X_val[i, :49, :]\n",
    "        y = X_val[i, 1:50, :]\n",
    "    elif test:\n",
    "        x = X_test[i, :49, :]\n",
    "        y = X_test[i, 1:50, :]\n",
    "    X.append(x)\n",
    "    X = np.array(X)\n",
    "    Y.append(y)\n",
    "    Y = np.array(Y)\n",
    "    Y_hat = seq.model.predict(X)\n",
    "    Y_hat_decoded = ae.decode_series(Y_hat)\n",
    "    Y_decoded = ae.decode_series(Y)\n",
    "    return Y_decoded[0], Y_hat_decoded[0], i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_videos_my_method(train=True, i=None):\n",
    "    if i == None:\n",
    "        i = random.randint(0, X_train.shape[0]-1)\n",
    "    X, Y = [], []\n",
    "    x, y = None, None\n",
    "    h, c = np.array([np.zeros(1024)]), np.array([np.zeros(1024)])\n",
    "    if train:\n",
    "        x = X_train[i, 0:3, :]\n",
    "        y = X_train[i, 3:83, :]\n",
    "    \n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    for j in range(80):\n",
    "        x_new, h_new, c_new = seq.forward.predict([x, h, c])\n",
    "        if j == 0:\n",
    "            x_new = x_new[:, 3-1:, :]\n",
    "        X.append(x_new)\n",
    "        x, h, c = x_new, h_new, c_new\n",
    "        \n",
    "    X = np.array(X)\n",
    "    Y.append(y)\n",
    "    Y = np.array(Y)\n",
    "        \n",
    "    X = np.reshape(X, (1, 80, 64))\n",
    "    Y_hat_decoded = ae.decode_series(X)\n",
    "    Y_decoded = ae.decode_series(Y)\n",
    "    print(Y_decoded.shape)\n",
    "    print(Y_hat_decoded.shape)\n",
    "#     result = np.concatenate((X, Y), axis=1)\n",
    "#     Y_decoded = ae.decode_series(result)\n",
    "    return Y_decoded[0], Y_hat_decoded[0], i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 80, 32, 32, 3)\n",
      "(1, 80, 32, 32, 3)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "Y, Y_hat, i = produce_videos_my_method(train=True)\n",
    "print(i)\n",
    "# i = 72, 127, 149, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAADXCAYAAABLRBVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALIElEQVR4nO3dP6hkZxkH4HPOzN2bZDeBxCxogoVEIURBtLEQQfyH2FgGgopFGgttFBQbESxstFAQxdZOS1tLC/8gUWMgEUVJEGPCZuMmu3fvzJljsYUJ+75fdubevfvOzPOU39yZM/fMnfnNgd99v36apg4Aqhru9BMAgBZBBUBpggqA0gQVAKUJKgBKE1QAlDZv3dj3ve46NEzT1N/Kz93J99JsuKWn+Car7Nk2/p3Fh8Wt67v4NZn2+Cy23kuuqAAoTVABUJqgAqA0QQVAaYIKgNKarb9d0eo87W/Hhl2T/Z33jdbffIi/q2atv9W4Sh9rtYpvy++xK/Lzu27fckjusdrzTypXVACUJqgAKE1QAVCaoAKgNEEFQGmCCoDS9qKevt/FTvZGH1ebh8bc3Avzw3D9+nQcrh8Ps/z4i2TQ6mqM1/NH2h3Ja5L98tmP93s+DNgVFQClCSoAShNUAJQmqAAoTVABUNpetP5gH0xJM2xMhsV2XdcdLeN23xCXAbthkT/WMi24Za3Dbeurxb9Hn1X1uq7LrgWyQcGraRmub9uZOm2uqAAoTVABUJqgAqA0QQVAaYIKgNK0/mDHjY05cdfHuGU2S+b2NQqEaeuw7+P1xtO6w7KBe+tuLN91U5fMOUzPY9mTcke5ogKgNEEFQGmCCoDSBBUApQkqAErT+oM9ls3hG5NW2rRqtNKSm6Z0h+GiDbdkDl8/xN/rpzFu9nVd1/XZ75g9VlaFLHqqzoorKgBKE1QAlCaoAChNUAFQmqACoDRBBUBp6umJ2XwWrmdbSKe10q7rxkVeX4XbrTVLNfurnVbJncb87zx/C2xXtzrdWj6rrWdDbLuu67Jqfp9sUZ+c32nLzuFpc0UFQGmCCoDSBBUApQkqAEoTVACUtvetv/mFu8L1x77xhXD9/ofeFa4//cOfpMe4/Kfnw/WxMcwSMvPk6+W9B3FT9XqjlLYc4/ssl/EW9au6+8efmilrNo7xOWl93e+zdl/S4ssah+MenPcWV1QAlCaoAChNUAFQmqACoDRBBUBpO9X6S2d0NRozD3/6I+H6Qx/7eHyMebxH9zs//3h6jMtf/158wxmU/rJTkrWRbtyYbJO9ip9wOodsv4tKJ5aMlusOZ/Hr88Bh/HY+ymqCXde98lp826JLGm57IX6Pb/LjU/Ymz95j3jQhV1QAlCaoAChNUAFQmqACoDRBBUBpggqA0jarpzeGXK7tFNuY6Xbwjb24L//lr+H6tUuvhOvnLr49XL/0+z+nx1glw2ezOv0mFdXssYZZ/BIfPPKB9LEeeOK74fri+X+G65d+9pVwfXV0JT3Gns/YPJFsd/Pu7njA7Hicn+xxdRwfY1qzos1a1j2/jY+w9DN0l95irqgAKE1QAVCaoAKgNEEFQGmCCoDS+rQp13Vd3/fJfsmNB0zW06Pc4WpKP8RZffdDF8P1cxfOh+v/fe4f6THWHdqa/Xw6dLdx2zA/CNcf/OYv0sc6/954IO94NT7Gyz/6Yrh+9Xc/T4+xGnejVTZNaQfvTdL3UkP2LXKeTKs9PIzvcX2RH2OxzAYNw9lqvZdcUQFQmqACoDRBBUBpggqA0gQVAKVtNOuv1V/KdjivOjpsWsVP7OoLL8brt/PJvJVGvyxrb2ZzBq89/Zv0se559KPh+vKly+H64l/PZE8qPQY3NCuDyY1jclpfP4r/lluzI71CbANXVACUJqgAKE1QAVCaoAKgNEEFQGnNWX9DMp/sg3ffk97nc/e/LVz/6aWXw/Vnjq61nh9vtMEun+lDNR7r4D0fjg9x5T/h+vji3+KfTxqVXbc7bbOTzvrbZLPsXTl38EZm/QGwtQQVAKUJKgBKE1QAlCaoAChNUAFQWrOe/uDsILzxV48+lt7n3ecOw/Wshv7J5+KBpq+uWiVcBd1Kkp3RmzNpp7SYvV2v7e3cih72iXo6AFtLUAFQmqACoDRBBUBpggqA0tpb0U/xNubdFDf7uq7r+v5cuH5+tgjXD2ZxVvar5NjdtvXCdl9W0GzX4byK+641GLlxr7V+utVq5mbrviRndXZdUQFQmqACoDRBBUBpggqA0gQVAKU1Z/1l88k+dP5Cep/PXnxHuP7Ll/4drv/22uvh+rgH25iz/cz6e2tDUu8bhvh78jDEzeGu67rsNC7GZbi+GrP28N6+HM2rkz55rabkfDVHsq7JrD8AtpagAqA0QQVAaYIKgNIEFQClbdT6azkYZuH6mBwnO37WMoFKtP7+r08mxc2Hg/jn+/h78sFB/v15NouPcbyM233Xj6+H6/s8A7D1B5uc3i7tTmr9AYCgAqA4QQVAaYIKgNIEFQClCSoASmtvRZ9obSG9TIbJqpvDfsoGTM/6+F9ZFstF+ljHSU86+3zJPqqa/1Sw4x9VrV9vWfR3d0UFQGmCCoDSBBUApQkqAEoTVACUtlHrrz2IsGhtBLitsubd1MXbxE9TXOHrx1YlL74tayJng3Jb39BXPsPKcUUFQGmCCoDSBBUApQkqAEoTVACUtlHrD+Ck0pZgo1actfiy79yzIf6IG6d4/uCNY2TzSpP7KAnedq6oAChNUAFQmqACoDRBBUBpggqA0gQVAKWppwNbI99yPq6OpwNm+/w7et/PkoOvX6fndLiiAqA0QQVAaYIKgNIEFQClCSoAStP6gzOQbZWeba3edY1Zp1pmN1kl56RfJetDo/WX3JSd9r7xcnipTocrKgBKE1QAlCaoAChNUAFQmqACoDStP05smCXbgDfmqS3HMVzf1blp6RbqeR2wG5Jm2mrPzt1JZKdkWi3T+/RT9lolj7Xmc2J9rqgAKE1QAVCaoAKgNEEFQGmCCoDStP64SdZQu3jx4XD9B9/5cbg+P7wrPcbXvvWlcP35F/4ero9J021bZI28vtH6m5I5ddp9p6A5ny8b6nd7ngpvzRUVAKUJKgBKE1QAlCaoAChNUAFQmqACoDT1dG7SD3EP94nHnwzXP/GZT8UP1KgAP/nMl8P1b3//q+H62G15PT1Z71tV82yP86wmrbV+e2UvR6u2ng2y9VqtxRUVAKUJKgBKE1QAlCaoAChNUAFQmtbfmpoNn8TWNXySX/LZZ/8Qrl95eRGuzw7y70FPPffHcH1sbBG+i6ZNqnpaf6W0PxLW29Z+o5bg1n3ArM8VFQClCSoAShNUAJQmqAAoTVABUFrf2ta677NhY3tsjxtXs/ksXH/fI+8P1+d3nU8f66mnfx2ur8ZVuF719E7TdEs9UO+l3dUc9ZfMzexn8XtpOBcXscdF/L7ouq6blskczClebz3f7I/0LIqFrfeSKyoAShNUAJQmqAAoTVABUJqgAqA0QQVAaerpcALq6bT02RTrWXyNMLv3MH6cZX5NsboaD4Xup3j9vnMX0se61h+H68vksZbHcQV+ytv0KfV0ALaWoAKgNEEFQGmCCoDSBBUApRXein6DPd/PYHRpPpN2k1GPwC5LW9Vj3JZbXjkK14cuHmJ74yBxxW7exx/vw/wgfajh4J5wvV+8Ft9h8Xq83iq5bvBx6IoKgNIEFQClCSoAShNUAJQmqAAorXDr78425dbfcV6zD7hF2cfFMm7wrbr1h+cd9/F9Xj26kt5nGO+LbxiTJ5w9rVP+OHRFBUBpggqA0gQVAKUJKgBKE1QAlGaHXzgBO/yyS/p+vWuXaZOtfNPHssMvAFtKUAFQmqACoDRBBUBpggqA0gQVAKUVHkoLwFk6zbr5aXJFBUBpggqA0gQVAKUJKgBKE1QAlNYcSgsAd5orKgBKE1QAlCaoAChNUAFQmqACoDRBBUBp/wMbW6ccXvr2+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show_images_as_video(Y_hat)\n",
    "compare_images_as_video(Y[0:, :], Y_hat[0:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 64)\n",
      "(1, 50, 64)\n",
      "(1, 50, 32, 32, 3)\n",
      "(1, 100, 32, 32, 3)\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "Y, Y_hat, i = produce_videos_new(train=True)\n",
    "print(i)\n",
    "# i = 18, 55, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAADXCAYAAABLRBVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANI0lEQVR4nO3dTY8s51UH8Krq6Zn7Zq6vnBgECU7AhAQJYckSLCIhJBALWLHKOixAYgErFixQvkQWCMSGD4DgC0QgsgALxJuQIytEJsHXL+Ta13c8ntvT3VUsDELknvN4uj135nTP77d8arqquvqpPl3Sf87TT9PUAUBVw1WfAAC0KFQAlKZQAVCaQgVAaQoVAKUpVACUdtDa2Pe97Do0TNPUn+fv3Etctr6Pn0OS4a7rum4+i6dzP4tftDxbh+Pr9dg+uUDrXvJEBUBpChUApSlUAJSmUAFQmkIFQGnN1B8AlydP6p0rXPoDr4n3dffezfQ1n/l8vO3DRZzi+6/Xj8PxR+8v0mNs0wjdExUApSlUAJSmUAFQmkIFQGkKFQClKVQAlCaeflFa6VHtSLkG+j6+CdLpv0VMed9l17D1JZK95MatW+H4l7780+m+Xnghfs2rf/d6OP72/P14R0Pjs4372DZ5ogKgNIUKgNIUKgBKU6gAKE2hAqA0qb+LIsBES5LMGlrrgmepuGR888W/t9BIt/YHycYk5SUN+KRxij/FVogu+9yHZPn4733nUbqv41mc4rv/4N1w/OThKhxP3sbWPFEBUJpCBUBpChUApSlUAJSmUAFQWt9aFrjv++sbv4FzmKbpXGuE90Oc28p7u3VddvdNSc7rUsJyjZ+28xuzcHxaxye2PovHt1mqnCdly9fPbs7T1xwexp/h4vQsHF8vNo105lr3kicqAEpTqAAoTaECoDSFCoDSFCoASpP6g0/g3Km/5F5qpf6aq0ZH5zI+/du11ZpwfhRvHOZxkmx9Gu9nuYr7x30kjUJyXq05l7mEJKbUHwA7S6ECoDSFCoDSFCoASlOoAChNoQKgNEvRX5RW5DNfc/tpnAk7pPnvIemGZEv2ggucZq0lxldJk9lhlYz3cWw9a6b60fGv8J5J7/HGOVW8xXew6a8nKgBKU6gAKE2hAqA0hQqA0hQqAEqT+ttYkvxpBGmGNC0Uj1/pUuOUt2mr6MuaNumS81O8XPnYZ/O8ES28ynsguwG36PHKZjxRAVCaQgVAaQoVAKUpVACUplABUJrU38bi5E/fqvlpa7YsQRjvK0sDsp/ylGeWPssm2uX0okt3lfTnm0o2wtvCnryNbVxCe8mu6zxRAVCcQgVAaQoVAKUpVACUplABUJpCBUBp4ukXZD7Ptz37fLzk9osvx5f/1VeW4fjDt/PQZ2tJc66H2SxpcjzE86/rum69jBvG6oB8PWVPLlMSRE/7bTf/I2LzueWJCoDSFCoASlOoAChNoQKgNIUKgNL6Vlqs7zdd9Hr/zZIA1e0fypNVv/WHd8Lx5z4fRwUffPcsHP/6H5ykx3j8YZLe2sK9z8XjP/fVePz7/xSP/9tfNA6SzKxdm3DTNJ1rIfKLvJfmSbrvmbvxfJrfiedf13Xdg7c+CMfXZ3HydG8ayV5jaVKv67p5nzTETj72YYh3tmo8Ao1Jk+JxtU7PzBMVAKUpVACUplABUJpCBUBpChUApen1t6F1shr8MMvTUAfzOMyyWMY7u5X8fBhaq91nq9onf5+ldbqu6371a/G2z76cvODX4qM8fD09RPef/yg99rGyleWTeZBlpm7dyK/1zdtxWvVkmaT+fGw7o0++FGatL5Ih/oD7JNy6nuLvsGnMv1+2CcB6ogKgNIUKgNIUKgBKU6gAKE2hAqA0qb9NJYGV4/eSOGDXdX/5Z3GPvi+9fBiO/8M3FuH42ePWCr/xeJb8aTX8up006no2mS2PzpLeXY/za9InkbbGO0y37K3kLa+SRNXicTx+cLZKD7F4HG+7hld7Z2V38iy5x+dZw9Ku61Zj3DN0nXzBpCnQJA3YdV13vu6Y/58nKgBKU6gAKE2hAqA0hQqA0hQqAEpTqAAozVL0lyGLY6bLO8fjY574zA+dRFTT2HrXdbfuxSfw0lfiv7+fNJj97iv5Ca/XSdw1fUVNV7EUffqZzjZbRrzruq5PrviYdV+mnCyGfpDF0xtNaeN/jMlj63n9aN0Wyb0/5veSJyoASlOoAChNoQKgNIUKgNIUKgBKk/rbc1mMpm8kfzZNCo5JHHFqNKbM4n27NuGeZuovC2YOQ9xUdByzJGXj0NaW3wmtSTbr43t53sfzpLUS/XKK032rZJqMaTfsxrxKvhZa95InKgBKU6gAKE2hAqA0hQqA0hQqAEqzFP2eS7M3zbRnPD4M8WvWSdqsGTbLN/E/ss8hT1le56u6YUPNvRK/xyT0160b1yRL8eVT63Liu56oAChNoQKgNIUKgNIUKgBKU6gAKE2vv2vqaJZ3D/vNlz4Vjv/M3aNw/M9fey8c/6s3TtJj7EtA7SpW+N01+QVKeko2XtHsW3gBf7+L0qt1rpl5PlO2swu8kfX6A2BnKVQAlKZQAVCaQgVAaQoVAKUpVACUpintNfUTdw/TbV/+sdvh+HoZL1P96y/eDce/+eZpeoxV1lj1EmKwpbQixHv6lv/XkDXdbbzxNG6+59eqZdN+sRd8lEvhiQqA0hQqAEpTqAAoTaECoDSFCoDSLi/1lyR8+my95EbKa5rixBjn98YHy3Tbaw8eh+NfuHcjHP/b+8fh+FZLo+9pui9bVr79onh41y5RdrqXk1ZjH3iiAqA0hQqA0hQqAEpTqAAoTaECoLSLX4o+iTfNDuKA4Z3PfiUcP7r3s+kh3nvtT8Px5fFrH3NynMdRkgV9/ma84c2TuAfg2Jg92bzbtcDXeZeinw1D/Nayhnddnu6bkj6JVWXvcEi+K9a7Fmts6NO4Zzx+nRPNlqIHYGcpVACUplABUJpCBUBpChUApSlUAJS2ZVPaPFKbpTGP7r0Ujv/wF38/HF/0ceS567puGO+E42//8++G49OY74snLVbx+PeOkw18vNnmvwmzu6xqeHvTvrvjjsXQs6j5MORfo7du3A7HsxD66elJ/Pdj3kT6OvBEBUBpChUApSlUAJSmUAFQmkIFQGlbpv4ajWyT7M968SgcXy4exvs5eC49xvLs1fg1SSpnt7JF7KO0NekwS1+TNZ9N5/mOpeiqnm12feezeTh+684z6b5e/IWXw/EPjj8Ix9/59hvh+PG7b6XHWK4W6bZ94YkKgNIUKgBKU6gAKE2hAqA0hQqA0rZM/eWy5FG2TPz9v//tcHx+58fTY5y+8zfh+LjWi46ahlmcJOv7PPs2JVlBrSufriFrWJp8hs9+7jDd13O/GG977v2fCsdPP4z3c7Y4TY+xevT9ZMvmuco8OXq1nSc9UQFQmkIFQGkKFQClKVQAlKZQAVDaU0j9bbZh8f63NhqHXTQu4759Bwd5r79xnawDe5U9/RrL+GY98rLzrdqacByT812eheNv/ce76b4OX/lOOH7nhfizPVkex8ee4mN3XdcNyWXPVlDO0qRdl3+G/Syep1k/ymx8W56oAChNoQKgNIUKgNIUKgBKU6gAKE2hAqC0vrV8dd/qmAl00zQ1Atv/ZxiG8F7KEt0f7Ts95nkO+VT0jZ+2fdK0NetbOq6zN7jZOV21YcgvysFRsu3wKBxeL+PG2uNimR5jyv6NIdOYdEMSQ88++CH5sNarvEF4Nn9b95InKgBKU6gAKE2hAqA0hQqA0hQqAEq78Ka0XD9pM9KGq0yuXYU86XTJJ/JJNc43mwXDM/N4w3GScGul2Aper7HRgPXsNNn2eB2PX8aEaCa9k3TfLC4V2bweuvyajF3y3hs8UQFQmkIFQGkKFQClKVQAlKZQAVCa1B/nl6T7sl5nn/nij6S7On5wEo4/fOdRON5KVnF5mqG0dTI/TuM/Pzq6EY6fLh7nh1jtyTwoGvdcr5Il75O+jLPkWWdsLHc/NLblrwGAwhQqAEpTqAAoTaECoDSFCoDSFCoAShNP59yy5rO/8tVfCsd/43d+Od3X22/GDUn/+Pf+KBx/49v32yfHlRuzBqVJovzGLPmdPORfSyd9HJ8umvbePcl1HKf4fp22iJpPW3QW9kQFQGkKFQClKVQAlKZQAVCaQgVAaVJ/PCkJ8hzMZuH4Cz//hXB8cXgzPcTzL8bprR/9yU+H4/f//c10X9dtWfuyko/hYIon1NE8/vpZNz7Pg1WcPltOe9KstqgsqZeN94004DZ3qycqAEpTqAAoTaECoDSFCoDSFCoASpP640lZLCdJY/31n3wjHP/0pw7TQ7zzrw/C8X/55reap0ZdWQLsdFqH4/Mx/p08y6dN1y2TcaG/Urbp59fiiQqA0hQqAEpTqAAoTaECoDSFCoDS+laftL7vNVG7AFnXq325uAdJz7ZWv69xjGNa62S86hKu05Q0svsB1/leGob49/B8Fs+bqYtTgl3Xdat1PD/G8dpe3r3Rupc8UQFQmkIFQGkKFQClKVQAlKZQAVCaQgVAaeLp8AmIp28vvXB9Yxnzov+mwCcnng7AzlKoAChNoQKgNIUKgNIUKgBKa6b+AOCqeaICoDSFCoDSFCoASlOoAChNoQKgNIUKgNL+G1dbZDSBrpHmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show_images_as_video(Y_hat)\n",
    "compare_images_as_video(Y[0:50, :], Y_hat[0:50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 64)\n",
      "(1, 50, 64)\n",
      "(1, 50, 32, 32, 3)\n",
      "(1, 100, 32, 32, 3)\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "Y, Y_hat, i = produce_videos_new_two(train=True, i=18)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_images_as_video(Y[0:50, :], Y_hat[0:50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 64)\n",
      "(1, 50, 64)\n",
      "(1, 50, 32, 32, 3)\n",
      "(1, 50, 32, 32, 3)\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "Y, Y_hat, i = produce_videos_new_three(train=True, i=18)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAADXCAYAAABLRBVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAE20lEQVR4nO3dLY9cVRzA4bn71hdaklZQjUVjkAjUfoN+gwZFgioGUVldiWnSpiQ0IUViCEn5JihQYMiWvUgQ597QycL+ZvZ55H929l6xJ785ydk70zzPGwCoOrjsGwCANUIFQJpQAZAmVACkCRUAaUIFQNrR2ovTNDm7DivmeZ7+zc9ZS7BubS3ZUQGQJlQApAkVAGlCBUCaUAGQJlQApAkVAGlCBUCaUAGQJlQApAkVAGlCBUDa6kNpAbh8a08+vgpPO7ajAiBNqABIEyoA0oQKgDShAiDNqT+AuKtwsm+NHRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZB2dNG/8OTkZDj/9OMHw/nPv/06nL/86cXiNc7ms7e/Mdgxx0fj5Xn/o9Ph/Gwef+785vV3i9f449xaos+OCoA0oQIgTagASBMqANKECoA0oQIgbavj6dPBtPjaF6cPh/PP7n8+nJ9vzofzN5vla7x8/XQ4nxffAVHLf+ab0w8/Gc4fP3gynB8ejz933rl+Y/EaT75/NpxbS5TYUQGQJlQApAkVAGlCBUCaUAGQttWpv4OVo0rHt28O54cn4/dMh+NWvndt+dyRE0nsi2llLd249e5wfnx9vGwPb4/n799d+Ty6dHmLjBA7KgDShAqANKECIE2oAEgTKgDStjr19+f5+Pl8m81m8/jrR8P5rYWvj//lze/D+Vc/Pn/7G4MdM8/Lx+te/fDtcP7B3TvD+c177wznX756sXL9lZuDCDsqANKECoA0oQIgTagASBMqANKmtVNH0zRd2JmgpWeazR4qxg6b53nlO3r/Zi3BurW1ZEcFQJpQAZAmVACkCRUAaUIFQJpQAZC21UNpt+HoLFwMa4mrxo4KgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiDt6LJvAP5pWpjP/+tdwO7bp7VkRwVAmlABkCZUAKQJFQBpQgVAmlN/pOziiSTgv2VHBUCaUAGQJlQApAkVAGlCBUCaUAGQ5ng6wB7ap3/1sKMCIE2oAEgTKgDShAqANKECIG2a5306GwLAvrGjAiBNqABIEyoA0oQKgDShAiBNqABI+wuK6GCS8BVdUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAADXCAYAAABLRBVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAE20lEQVR4nO3dLY9cVRzA4bn71hdaklZQjUVjkAjUfoN+gwZFgioGUVldiWnSpiQ0IUViCEn5JihQYMiWvUgQ597QycL+ZvZ55H929l6xJ785ydk70zzPGwCoOrjsGwCANUIFQJpQAZAmVACkCRUAaUIFQNrR2ovTNDm7DivmeZ7+zc9ZS7BubS3ZUQGQJlQApAkVAGlCBUCaUAGQJlQApAkVAGlCBUCaUAGQJlQApAkVAGlCBUDa6kNpAbh8a08+vgpPO7ajAiBNqABIEyoA0oQKgDShAiDNqT+AuKtwsm+NHRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZAmVACkCRUAaUIFQJpQAZB2dNG/8OTkZDj/9OMHw/nPv/06nL/86cXiNc7ms7e/Mdgxx0fj5Xn/o9Ph/Gwef+785vV3i9f449xaos+OCoA0oQIgTagASBMqANKECoA0oQIgbavj6dPBtPjaF6cPh/PP7n8+nJ9vzofzN5vla7x8/XQ4nxffAVHLf+ab0w8/Gc4fP3gynB8ejz933rl+Y/EaT75/NpxbS5TYUQGQJlQApAkVAGlCBUCaUAGQttWpv4OVo0rHt28O54cn4/dMh+NWvndt+dyRE0nsi2llLd249e5wfnx9vGwPb4/n799d+Ty6dHmLjBA7KgDShAqANKECIE2oAEgTKgDStjr19+f5+Pl8m81m8/jrR8P5rYWvj//lze/D+Vc/Pn/7G4MdM8/Lx+te/fDtcP7B3TvD+c177wznX756sXL9lZuDCDsqANKECoA0oQIgTagASBMqANKmtVNH0zRd2JmgpWeazR4qxg6b53nlO3r/Zi3BurW1ZEcFQJpQAZAmVACkCRUAaUIFQJpQAZC21UNpt+HoLFwMa4mrxo4KgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiBNqABIEyoA0oQKgDShAiDt6LJvAP5pWpjP/+tdwO7bp7VkRwVAmlABkCZUAKQJFQBpQgVAmlN/pOziiSTgv2VHBUCaUAGQJlQApAkVAGlCBUCaUAGQ5ng6wB7ap3/1sKMCIE2oAEgTKgDShAqANKECIG2a5306GwLAvrGjAiBNqABIEyoA0oQKgDShAiBNqABI+wuK6GCS8BVdUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_images_as_video(Y[0:50, :], Y_hat[0:50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, Y_hat, _ = produce_videos(train=True, i=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAADXCAYAAABLRBVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFCklEQVR4nO3cv4oeVRyA4UzYjVkxSASJhXYipDDeQRotBP+A9yAELQzoVVhYpNHKUgS9AMVIQCsRTCUJ1iaVuiKSaHZjxlbkzBezfu6+G5+n/M1+M9Mc3j1wmGme5yMAUHX0oF8AAFYRKgDShAqANKECIE2oAEgTKgDSNlZdnKbJ2XVYYZ7n6Z/8nbUEq61aS3ZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpG/v1oGmahvMHNjeH852d24v3unPkzlreCQ6jpbV0/Nix4XxnZ3fxXn/M1hJ9dlQApAkVAGlCBUCaUAGQJlQApAkVAGlrP55+dOHo7JsvvDGcv/r8ueH8vS8/WHzGux+/PZzvzstH2uGwWTqGfv6l14fzcy++Npy/f+nDxWdc+GhhLd2xluiwowIgTagASBMqANKECoA0oQIgbZrnefniNC1fXPDYiVPD+eV3vhnOT249PJzfvL2z+Iwzb50Zzq9vX7/L28F6zfM8Ppr3N3tZS6dOPDqcX74wXkuPPHRyOP/t1vJaevr8M8P59e1rd3k7WK9Va8mOCoA0oQIgTagASBMqANKECoC0tX/r79ebPw/nX129NJw/d/bl4fyzi58sPmP7xo/3/mJwyNy4+ctw/vWVz4fzZ8++Mpxf/OLTxWf8dOOHe38x2Gd2VACkCRUAaUIFQJpQAZAmVACkrf1bf0u2NreG89NPPDWcX/n+6uK9ft9d/nYZ7Kf/8lt/Sx7cPD6cn378yeH822vfLd7r1u7uWt4J/i3f+gPg0BIqANKECoA0oQIgTagASBMqANL27Xg63I8O4ng63I8cTwfg0BIqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANI2DvoF4K+W/nOaV/xm1TX4v5r28JvqWrKjAiBNqABIEyoA0oQKgDShAiDNqT9S9nLqaOl0U/UEE1RV15IdFQBpQgVAmlABkCZUAKQJFQBpQgVAmuPpHHoHfXQW7hfVtWRHBUCaUAGQJlQApAkVAGlCBUDaNM/Vcx4AYEcFQJxQAZAmVACkCRUAaUIFQJpQAZD2Jye7ibEYbCm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAADXCAYAAABLRBVdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAFCklEQVR4nO3cv4oeVRyA4UzYjVkxSASJhXYipDDeQRotBP+A9yAELQzoVVhYpNHKUgS9AMVIQCsRTCUJ1iaVuiKSaHZjxlbkzBezfu6+G5+n/M1+M9Mc3j1wmGme5yMAUHX0oF8AAFYRKgDShAqANKECIE2oAEgTKgDSNlZdnKbJ2XVYYZ7n6Z/8nbUEq61aS3ZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpQgVAmlABkCZUAKQJFQBpG/v1oGmahvMHNjeH852d24v3unPkzlreCQ6jpbV0/Nix4XxnZ3fxXn/M1hJ9dlQApAkVAGlCBUCaUAGQJlQApAkVAGlrP55+dOHo7JsvvDGcv/r8ueH8vS8/WHzGux+/PZzvzstH2uGwWTqGfv6l14fzcy++Npy/f+nDxWdc+GhhLd2xluiwowIgTagASBMqANKECoA0oQIgbZrnefniNC1fXPDYiVPD+eV3vhnOT249PJzfvL2z+Iwzb50Zzq9vX7/L28F6zfM8Ppr3N3tZS6dOPDqcX74wXkuPPHRyOP/t1vJaevr8M8P59e1rd3k7WK9Va8mOCoA0oQIgTagASBMqANKECoC0tX/r79ebPw/nX129NJw/d/bl4fyzi58sPmP7xo/3/mJwyNy4+ctw/vWVz4fzZ8++Mpxf/OLTxWf8dOOHe38x2Gd2VACkCRUAaUIFQJpQAZAmVACkrf1bf0u2NreG89NPPDWcX/n+6uK9ft9d/nYZ7Kf/8lt/Sx7cPD6cn378yeH822vfLd7r1u7uWt4J/i3f+gPg0BIqANKECoA0oQIgTagASBMqANL27Xg63I8O4ng63I8cTwfg0BIqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANKECoA0oQIgTagASBMqANI2DvoF4K+W/nOaV/xm1TX4v5r28JvqWrKjAiBNqABIEyoA0oQKgDShAiDNqT9S9nLqaOl0U/UEE1RV15IdFQBpQgVAmlABkCZUAKQJFQBpQgVAmuPpHHoHfXQW7hfVtWRHBUCaUAGQJlQApAkVAGlCBUDaNM/Vcx4AYEcFQJxQAZAmVACkCRUAaUIFQJpQAZD2Jye7ibEYbCm9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_images_as_video(Y[0:50, :], Y_hat[0:50, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_long_videos(train=True, val=False, test=False):\n",
    "    i = 0\n",
    "    if train:\n",
    "        i = random.randint(0, X_train.shape[0]-1)\n",
    "    elif val:\n",
    "        i = random.randint(0, X_val.shape[0]-1)\n",
    "    elif test:\n",
    "        i = random.randint(0, X_test.shape[0]-1)\n",
    "    X, Y = [], []\n",
    "    x, y = None, None\n",
    "    if train:\n",
    "        x = X_train[i, :20, :]\n",
    "        y = X_train[i, 20:50, :]\n",
    "    elif val:\n",
    "        x = X_val[i, :20, :]\n",
    "        y = X_val[i, 20:50, :]\n",
    "    elif test:\n",
    "        x = X_test[i, :20, :]\n",
    "        y = X_test[i, 20:50, :]\n",
    "    X.append(x)\n",
    "    X = np.array(X)\n",
    "    Y_hat = seq.model.predict(X)\n",
    "    result = np.concatenate((X, Y_hat), axis=1)\n",
    "    for j in range(1):\n",
    "        X_in = result[:, -20:, :]\n",
    "        Y_hat_out = seq.model.predict(X_in)\n",
    "        result = np.concatenate((result, Y_hat_out), axis=1)    \n",
    "    print(result.shape)\n",
    "    Y_hat_decoded = ae.decode_series(result)\n",
    "    if train:\n",
    "        y = X_train[i, :result.shape[1], :]\n",
    "    elif val:\n",
    "        y = X_val[i, :result.shape[1], :]\n",
    "    elif test:\n",
    "        y = X_test[i, :result.shape[1], :]\n",
    "    Y.append(y)\n",
    "    Y = np.array(Y)    \n",
    "    Y_decoded = ae.decode_series(Y)\n",
    "    return Y_decoded[0], Y_hat_decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 80, 64)\n"
     ]
    }
   ],
   "source": [
    "Y_long, Y_hat_long = produce_long_videos(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_images_as_video(Y_long, Y_hat_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nConclusion:\\n\\nWe can learn a one to one mapping that predicts t+1 given t. However, if we just predict t+1 from t alone, the model fails\\nto roll out the future. However, if we initialize the model with the hidden and cell state from the last time step\\nwe can accurately predict t+1 give t, h, and c. This is the interesting result of an RNN--a CNN or different architecture\\ncould not unroll the future because it has no idea of hidden state and cell state. In an RNN, the hidden state and cell state\\nare updated with the trained weights in order to make a prediction for the next time step. Initialing our model's input\\nwith t and the updated h and c from the model's last output is critical for an accurate prediction of t+1.\\n\\nThe model can unroll the future given just a single timestep, hidden state, and cell state. However, it fails to work well\\non input with one, two, three balls. It works best on more balls. This leads me to believe that it should be trained with \\nhigher variance and higher quantity of data.\\n\""
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Conclusion:\n",
    "\n",
    "We can learn a one to one mapping that predicts t+1 given t. However, if we just predict t+1 from t alone, the model fails\n",
    "to roll out the future. However, if we initialize the model with the hidden and cell state from the last time step\n",
    "we can accurately predict t+1 give t, h, and c. This is the interesting result of an RNN--a CNN or different architecture\n",
    "could not unroll the future because it has no idea of hidden state and cell state. In an RNN, the hidden state and cell state\n",
    "are updated with the trained weights in order to make a prediction for the next time step. Initialing our model's input\n",
    "with t and the updated h and c from the model's last output is critical for an accurate prediction of t+1.\n",
    "\n",
    "The model can unroll the future given just a single timestep, hidden state, and cell state. However, it fails to work well\n",
    "on input with one, two, three balls. It works best on more balls. This leads me to believe that it should be trained with \n",
    "higher variance and higher quantity of data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
